const { Pinecone } = require('@pinecone-database/pinecone')

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'Content-Type',
  'Access-Control-Allow-Methods': 'POST, OPTIONS'
}

exports.handler = async (event, context) => {
  console.log('ğŸ’¬ Chat function ì‹œì‘')
  
  // Handle CORS preflight
  if (event.httpMethod === 'OPTIONS') {
    console.log('ğŸ“‹ CORS preflight ìš”ì²­ ì²˜ë¦¬')
    return {
      statusCode: 200,
      headers: corsHeaders,
      body: ''
    }
  }

  if (event.httpMethod !== 'POST') {
    console.log('âŒ POSTê°€ ì•„ë‹Œ ìš”ì²­:', event.httpMethod)
    return {
      statusCode: 405,
      headers: corsHeaders,
      body: JSON.stringify({ error: 'Method not allowed' })
    }
  }

  try {
    const { message, history } = JSON.parse(event.body)
    console.log('ğŸ“ ì‚¬ìš©ì ì§ˆë¬¸:', message)
    console.log('ğŸ“š ëŒ€í™” íˆìŠ¤í† ë¦¬ ê¸¸ì´:', history?.length || 0)

    if (!message) {
      console.log('âŒ ë©”ì‹œì§€ê°€ ë¹„ì–´ìˆìŒ')
      return {
        statusCode: 400,
        headers: corsHeaders,
        body: JSON.stringify({ error: 'Message is required' })
      }
    }

    // Initialize Solar LLM client
    const solarApiKey = process.env.UPSTAGE_API_KEY
    if (!solarApiKey) {
      throw new Error('UPSTAGE_API_KEY not configured')
    }

    // 1. Generate embedding for the user query
    console.log('ğŸ”® ì§ˆë¬¸ ì„ë² ë”© ìƒì„± ì‹œì‘...')
    const embeddingResponse = await fetch('https://api.upstage.ai/v1/solar/embeddings', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${solarApiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'solar-embedding-1-large-query',
        input: message
      })
    })

    console.log('ğŸ“ ì„ë² ë”© API ì‘ë‹µ:', embeddingResponse.status)

    if (!embeddingResponse.ok) {
      console.error('âŒ ì„ë² ë”© API ì˜¤ë¥˜:', embeddingResponse.status)
      throw new Error(`Embedding API error: ${embeddingResponse.status}`)
    }

    const embeddingData = await embeddingResponse.json()
    const queryEmbedding = embeddingData.data[0].embedding
    console.log('âœ… ì§ˆë¬¸ ì„ë² ë”© ìƒì„± ì™„ë£Œ')

    // 2. Search similar documents in Pinecone
    console.log('ğŸ” ë²¡í„° ê²€ìƒ‰ ì‹œì‘...')
    let sources = []
    const pineconeApiKey = process.env.PINECONE_API_KEY
    const pineconeIndex = process.env.PINECONE_INDEX

    if (pineconeApiKey && pineconeIndex) {
      try {
        const pinecone = new Pinecone({
          apiKey: pineconeApiKey
        })

        const index = pinecone.index(pineconeIndex)
        console.log(`ğŸ”— Pinecone ì¸ë±ìŠ¤ ì—°ê²°: ${pineconeIndex}`)
        
        const searchResults = await index.query({
          vector: queryEmbedding,
          topK: 5,
          includeMetadata: true
        })

        console.log(`ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: ${searchResults.matches.length}ê°œ ë¬¸ì„œ ë°œê²¬`)
        
        sources = searchResults.matches.map(match => ({
          content: match.metadata?.text || '',
          score: match.score,
          page: match.metadata?.page
        }))

        console.log('ğŸ¯ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤:')
        sources.forEach((source, i) => {
          console.log(`  ${i+1}. ì ìˆ˜: ${source.score.toFixed(3)}, ê¸¸ì´: ${source.content.length}ì`)
        })

      } catch (pineconeError) {
        console.error('âŒ Pinecone ê²€ìƒ‰ ì‹¤íŒ¨:', pineconeError)
        // Continue without sources
      }
    } else {
      console.log('âš ï¸ Pinecone ì„¤ì •ì´ ì—†ì–´ ê²€ìƒ‰ ê±´ë„ˆëœ€')
    }

    // 3. Build context from retrieved documents
    console.log('ğŸ“ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...')
    const context = sources.length > 0 
      ? sources.map(s => s.content).join('\n\n')
      : 'No relevant documents found.'

    console.log(`ğŸ“„ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: ${context.length}ì`)
    console.log(`ğŸ“‹ ì‚¬ìš©í•  ì†ŒìŠ¤ ê°œìˆ˜: ${sources.length}ê°œ`)

    // 4. Generate response using Solar LLM
    console.log('ğŸ¤– LLM ì‘ë‹µ ìƒì„± ì‹œì‘...')
    const systemPrompt = `You are a helpful AI assistant that answers questions based on the provided context. If the context doesn't contain relevant information, say so honestly.

Context:
${context}

Please provide a comprehensive answer based on the context above.`

    const messages = [
      { role: 'system', content: systemPrompt },
      ...history.map(h => ({ role: h.role, content: h.content })),
      { role: 'user', content: message }
    ]

    console.log(`ğŸ“¨ ë©”ì‹œì§€ ê°œìˆ˜: ${messages.length}ê°œ`)

    const chatResponse = await fetch('https://api.upstage.ai/v1/solar/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${solarApiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'solar-pro2-preview',
        messages: messages,
        temperature: 0.1,
        max_tokens: 1000
      })
    })

    console.log('ğŸ“ ì±„íŒ… API ì‘ë‹µ:', chatResponse.status)

    if (!chatResponse.ok) {
      console.error('âŒ ì±„íŒ… API ì˜¤ë¥˜:', chatResponse.status)
      throw new Error(`Chat API error: ${chatResponse.status}`)
    }

    const chatData = await chatResponse.json()
    const response = chatData.choices[0].message.content

    console.log('âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ')
    console.log(`ğŸ“ ì‘ë‹µ ê¸¸ì´: ${response.length}ì`)

    const filteredSources = sources.filter(s => s.score > 0.5) // Lowered from 0.7 to 0.5 for more results
    console.log(`ğŸ¯ ê³ ì‹ ë¢°ë„ ì†ŒìŠ¤ ê°œìˆ˜: ${filteredSources.length}ê°œ (ì„ê³„ê°’ 0.5 ì´ìƒ)`)

    return {
      statusCode: 200,
      headers: corsHeaders,
      body: JSON.stringify({
        response,
        sources: filteredSources // Only return high-confidence sources
      })
    }

  } catch (error) {
    console.error('ğŸ’¥ Chat function ì˜¤ë¥˜:', error)
    console.error('ì˜¤ë¥˜ ìŠ¤íƒ:', error.stack)
    return {
      statusCode: 500,
      headers: corsHeaders,
      body: JSON.stringify({ 
        error: 'Internal server error',
        message: error.message 
      })
    }
  }
}